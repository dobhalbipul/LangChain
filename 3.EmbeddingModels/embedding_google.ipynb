{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2daf567",
   "metadata": {},
   "source": [
    "### Full RAG Pipeline with Gemini (Google Generative AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14be47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ab43003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS # A simple in-memory vector store\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e60e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # Load your API key from .env\n",
    "\n",
    "# 1. Initialize embedding model\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a792be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic Embedding Usage ---\n",
    "print(\"\\n--- Basic Embedding Usage ---\")\n",
    "text = \"LangChain makes building LLM applications easier.\"\n",
    "embedding = embeddings_model.embed_query(text)\n",
    "print(f\"Embedding dimensions: {len(embedding)}\")\n",
    "print(f\"First 5 dimensions: {embedding[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064327cf",
   "metadata": {},
   "source": [
    "### Using Embeddings with a Vector Store (Example: FAISS for RAG) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c002f339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using Embeddings with a Vector Store (for RAG) ---\n",
      "Embedding vector length: 768\n",
      "First 5 values of first embedding: [0.057657234370708466, 0.010308874770998955, -0.044133931398391724, 0.009841084480285645, 0.0408058762550354]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Using Embeddings with a Vector Store (for RAG) ---\")\n",
    "texts = [\"LangChain integrates with Gemini.\", \"Embeddings power retrieval-augmented generation.\"]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embeddings_model.embed_documents(texts)\n",
    "\n",
    "print(\"Embedding vector length:\", len(embeddings[0]))\n",
    "print(\"First 5 values of first embedding:\", embeddings[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e6c929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sample knowledge base\n",
    "texts = [\n",
    "    \"LangChain is a framework for developing applications powered by LLMs.\",\n",
    "    \"Gemini is a family of large language models developed by Google DeepMind.\",\n",
    "    \"FAISS is a library for efficient similarity search and clustering of dense vectors.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be297603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Wrap into LangChain Documents\n",
    "documents = [Document(page_content=txt) for txt in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e7efeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(documents, embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ce6ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match 1: Gemini is a family of large language models developed by Google DeepMind.\n",
      "Match 2: FAISS is a library for efficient similarity search and clustering of dense vectors.\n"
     ]
    }
   ],
   "source": [
    "# Run a Similarity Search\n",
    "query = \"What is Gemini?\"\n",
    "results = vectorstore.similarity_search(query, k=2) # Get top 2 results\n",
    "\n",
    "for idx, doc in enumerate(results):\n",
    "    print(f\"Match {idx+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9635e15",
   "metadata": {},
   "source": [
    "### Use Gemini-Pro for Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7b10009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Gemini is a family of large language models developed by Google DeepMind.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "# Initialize Gemini LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n",
    "\n",
    "# Setup RAG pipeline using Retriever + LLM\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, # Use the Gemini LLM\n",
    "    retriever=vectorstore.as_retriever(), # Use the FAISS vector store as retriever\n",
    "    return_source_documents=True # Return source documents for context\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "query = \"What is Gemini?\"\n",
    "response = rag_chain({\"query\": query})\n",
    "\n",
    "# Print the answer\n",
    "print(\"Answer:\", response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87735937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: FAISS is a library for efficient similarity search and clustering of dense vectors.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "query = \"What is FAISS?\"\n",
    "response = rag_chain({\"query\": query})\n",
    "\n",
    "# Print the answer\n",
    "print(\"Answer:\", response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dcb40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: LLM stands for Large Language Model.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question - which is not in the knowledge base\n",
    "query = \"What is LLM?\"\n",
    "response = rag_chain({\"query\": query})\n",
    "\n",
    "# Print the answer\n",
    "print(\"Answer:\", response[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
