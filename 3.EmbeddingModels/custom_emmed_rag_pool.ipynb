{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8a29fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bdobhalx\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.vectorstores import FAISS # Using FAISS for efficient in-memory vector search\n",
    "from langchain_core.documents import Document # To structure our data for LangChain\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a01955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "csv_file_path = 'tickets.csv'\n",
    "issue_column_name = 'Issue'\n",
    "resolution_column_name = 'Resolution'\n",
    "embedding_model_name = 'mixedbread-ai/mxbai-embed-large-v1'\n",
    "vector_db_path = \"faiss_index_pool\" # Directory to save/load FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2264ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading data from 'tickets.csv' ---\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Loading data from '{csv_file_path}' ---\")\n",
    "if not os.path.exists(csv_file_path):\n",
    "    print(f\"Error: The file '{csv_file_path}' was not found.\")\n",
    "    print(\"Please ensure the CSV file is in the same directory as this notebook or provide the full path.\")\n",
    "    # You might want to upload your CSV via VS Code's explorer if it's not there.\n",
    "    # If running in Colab, use files.upload() or mount Google Drive.\n",
    "    raise FileNotFoundError(f\"CSV file not found: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3651a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue</th>\n",
       "      <th>Resolution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Run solve having error</td>\n",
       "      <td>HXVaf have nsmusd for 2 conversion groups, whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OneMPS does not have RTF for 2025Q1 for all so...</td>\n",
       "      <td>RC: extra 2025Q1 quarter in weekly version du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Error to pull Limiter Chart By VGG report. FVL...</td>\n",
       "      <td>The issue has been fixed. Pls help to verify -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>can't save \" modify conversion group'</td>\n",
       "      <td>Explaination provided: Need to save record fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>HX_CT_MB_ARROW_LAKE_H_6C+8A+GT2_N3B_HFE is co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Issue  \\\n",
       "0                             Run solve having error   \n",
       "1  OneMPS does not have RTF for 2025Q1 for all so...   \n",
       "2  Error to pull Limiter Chart By VGG report. FVL...   \n",
       "3              can't save \" modify conversion group'   \n",
       "4                                                NaN   \n",
       "\n",
       "                                          Resolution  \n",
       "0  HXVaf have nsmusd for 2 conversion groups, whi...  \n",
       "1   RC: extra 2025Q1 quarter in weekly version du...  \n",
       "2    The issue has been fixed. Pls help to verify -   \n",
       "3  Explaination provided: Need to save record fir...  \n",
       "4   HX_CT_MB_ARROW_LAKE_H_6C+8A+GT2_N3B_HFE is co...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. Load Data ---\n",
    "df = pd.read_csv(csv_file_path)\n",
    "required_columns = [issue_column_name, resolution_column_name]\n",
    "\n",
    "df = df[required_columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a50c3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 334 documents for LangChain.\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for LangChain Documents\n",
    "# Each document will have the issue description as page_content\n",
    "# And the resolution as part of its metadata.\n",
    "documents = []\n",
    "for index, row in df.iterrows():\n",
    "    # Using the issue description as the main content for embedding\n",
    "    page_content = row[issue_column_name]\n",
    "    # Skip rows where issue description is missing or not a string\n",
    "    if not isinstance(page_content, str) or pd.isna(page_content):\n",
    "        continue\n",
    "    # Storing resolution and original index in metadata\n",
    "    metadata = {\n",
    "        \"resolution\": row[resolution_column_name],\n",
    "        \"original_index\": index,\n",
    "        \"issue_description\": row[issue_column_name] # Also good to have original issue in metadata\n",
    "    }\n",
    "    documents.append(Document(page_content=page_content, metadata=metadata))\n",
    "\n",
    "print(f\"Successfully loaded {len(documents)} documents for LangChain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6154f6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Load Embedding Model ---\n",
    "# SentenceTransformer directly wraps the Hugging Face model for embeddings\n",
    "# LangChain's SentenceTransformerEmbeddings expects a model that can be loaded this way\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "embed_model = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n",
    "print(\"Embedding model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ee967c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating new FAISS index and generating embeddings ---\n",
      "FAISS index created and saved to 'faiss_index_pool'.\n",
      "Retriever configured.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Create and Persist/Load Vector Store (FAISS) ---\n",
    "# This step generates embeddings and builds the search index.\n",
    "# We'll save it to disk so we don't have to re-embed every time.\n",
    "\n",
    "if os.path.exists(vector_db_path):\n",
    "    print(f\"\\n--- Loading existing FAISS index from '{vector_db_path}' ---\")\n",
    "    vectorstore = FAISS.load_local(vector_db_path, embed_model, allow_dangerous_deserialization=True)\n",
    "    print(\"FAISS index loaded.\")\n",
    "else:\n",
    "    print(\"\\n--- Creating new FAISS index and generating embeddings ---\")\n",
    "    # This step will take some time depending on data size and GPU availability.\n",
    "    # FAISS.from_documents takes care of encoding documents using the embed_model\n",
    "    vectorstore = FAISS.from_documents(documents, embed_model)\n",
    "    vectorstore.save_local(vector_db_path)\n",
    "    print(f\"FAISS index created and saved to '{vector_db_path}'.\")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # Retrieve top 2 relevant documents\n",
    "print(\"Retriever configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6cdbb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bdobhalx\\AppData\\Local\\Temp\\ipykernel_25012\\3424088213.py:8: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"gemma3:1b\")\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Initialize Ollama LLM ---\n",
    "# Set up Ollama LLM and LangChain RAG Chain\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = Ollama(model=\"gemma3:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "afa516d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Define the Prompt Template ---\n",
    "# This is for guiding the LLM to use the context or output the fallback.\n",
    "prompt_template = \"\"\"\n",
    "You are a helpful IT support assistant. Your goal is to provide concise and accurate solutions to technical issues.\n",
    "You will be provided with a user's technical issue and relevant context (Issue Description and Resolution) from our knowledge base.\n",
    "\n",
    "If the provided context contains relevant information to address the user's input, synthesize the best possible solution from the 'Resolution' in the context.\n",
    "Prioritize the provided resolution in your answer.\n",
    "\n",
    "\n",
    "If the provided context contains multiple resolutions, combine them and provide bulleted list of probable solutions.\n",
    "Keep the response as professional support assistant.\n",
    "Correct the spelling, grammer in the solution and paraphrase always, if needed.\n",
    "If the context does NOT contain any information directly relevant to the user's input, or if you cannot form a solution from the provided context,\n",
    "then you MUST respond with: \"Please raise a pool ticket for this issue.\" \n",
    "User's Input: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Solution:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae21c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Set up the RAG Chain ---\n",
    "# Define a format_docs function to prepare retrieved documents for the prompt\n",
    "def format_docs(docs):\n",
    "    # This function extracts relevant parts from the Document objects\n",
    "    # and formats them into a string for the LLM context.\n",
    "    formatted_context = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        # We put both the 'issue_description' and 'resolution' into the context for the LLM\n",
    "        # So the LLM can see both the original problem and its solution.\n",
    "        formatted_context += f\"Issue {i+1}: {doc.metadata['issue_description']}\\n\"\n",
    "        formatted_context += f\"Resolution {i+1}: {doc.metadata['resolution']}\\n\\n\"\n",
    "    return formatted_context.strip() # Remove any trailing newlines\n",
    "\n",
    "\n",
    "# Build the RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c6c5da85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "query: 'My VPN is not connecting.': ---\n",
      "response: Okay, let's try to resolve this VPN connectivity issue.\n",
      "\n",
      "Based on the provided context, here’s a possible solution:\n",
      "\n",
      "*   **Restart the PC:** This often resolves temporary network issues.\n",
      "*   **Connect GlobalProtect:**  Attempting to connect via GlobalProtect might help establish a VPN connection.\n",
      "*   **Check the VPN account:** Ensure your VPN account is properly configured and active.\n",
      "*   **Verify the limiter:** Double-check the limiter settings to ensure the VPN connection is allowed.\n",
      "*   **Check PG8_VGG_CMT_TIU_APN490_94_97:** Verify this limiter is set to a sufficient capacity.\n",
      "\n",
      "**Please raise a pool ticket for this issue.**\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG system\n",
    "query1 = \"My VPN is not connecting.\"\n",
    "response1 = rag_chain.invoke(query1)\n",
    "response1 = response1.strip()  # Clean up the response\n",
    "print(f\"\\nquery: '{query1}': ---\\nresponse: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1962edbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "query: 'I cannot access ISA, looks like access issue.': ---\n",
      "response: Okay, here’s a solution to your issue:\n",
      "\n",
      "**Possible Solutions:**\n",
      "\n",
      "*   Check the availability of the PSICapGroupGroup Available Inventory in W41/50/51.\n",
      "*   Verify that the PSICapGroup is populated for PSIGCapgroup.\n",
      "*   Ensure the PSIG data is being loaded from the EMS source.\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG system\n",
    "query2 = \"I cannot access ISA, looks like access issue.\"\n",
    "response2 = rag_chain.invoke(query2)\n",
    "response2 = response2.strip()  # Clean up the response\n",
    "print(f\"\\nquery: '{query2}': ---\\nresponse: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca5272d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "query: 'EC Solve failed': ---\n",
      "response: Okay, let's address the EC Solve failure.\n",
      "\n",
      "Based on the provided context, the best solution is:\n",
      "\n",
      "*   **Change the finish yield2 in frozen horizon to 1.**\n",
      "\n",
      "Here’s a breakdown of potential related solutions:\n",
      "\n",
      "*   **VGG cannot handle NST flow prod, especially in frozen.**  This suggests a potential issue with the VGG model's ability to process the NST flow in the frozen state.\n",
      "*   **Further check and fix by the tech team.**  The context indicates the tech team needs to investigate and resolve this issue.\n",
      "*   **NSF yield on the 2nd stage is double counted, tech team will further check and fix it.** This is a potential cause of the problem, and the fix will involve investigating and resolving it.\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG system\n",
    "query3 = \"EC Solve failed\"\n",
    "response3 = rag_chain.invoke(query3)\n",
    "response3 = response3.strip()  # Clean up the response\n",
    "print(f\"\\nquery: '{query3}': ---\\nresponse: {response3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "41f514e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "query: 'Missing Cap Group in PSI Cap Group Mapping': ---\n",
      "response: Okay, I understand. Here's a breakdown of the issue and a proposed solution:\n",
      "\n",
      "**Issue:** Missing Cap Group in PSI Cap Group Mapping\n",
      "\n",
      "**Analysis:** The user is reporting that a \"Cap Group\" is missing in a PSI Cap Group Mapping. This likely impacts the functionality of the mapping and could lead to data discrepancies.\n",
      "\n",
      "**Proposed Solution:** Verify that the Cap Group assigned to the PSI Cap Group Mapping is correctly populated and that the data is being loaded as expected.\n",
      "\n",
      "**Possible Solutions from the Provided Resolution:**\n",
      "\n",
      "*   **Resolution 1:** Re-load the data from VG.\n",
      "*   **Resolution 2:** Re-load the data with the older start date.\n",
      "*   **Resolution 3:** Ensure the Cap Group is correctly selected in the filter.\n",
      "\n",
      "**Therefore, the best solution is to re-load the data from VG, as it was the most recent update.**\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG system\n",
    "query4 = \"Missing Cap Group in PSI Cap Group Mapping\"\n",
    "response4 = rag_chain.invoke(query4)\n",
    "response4 = response4.strip()  # Clean up the response\n",
    "print(f\"\\nquery: '{query4}': ---\\nresponse: {response4}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
