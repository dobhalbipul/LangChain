{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c634183",
   "metadata": {},
   "source": [
    "**RAG (Retrieval-Augmented Generation)** is a powerful technique that combines the strengths of retrieval systems (like vector databases storing embeddings) with large language models (LLMs). The goal is to provide the LLM with relevant, external information so it can generate more accurate, up-to-date, and grounded responses, reducing hallucinations.\n",
    "\n",
    "Here's an example of a RAG pipeline using LangChain, with gemini-embedding-001 for embeddings and gemini-1.5-pro for generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a386f4",
   "metadata": {},
   "source": [
    "**Core Components of a RAG Pipeline:**\n",
    "\n",
    "- Document Loading: Loading data (PDFs, text files, web pages, etc.).\n",
    "- Document Splitting (Chunking): Breaking down large documents into smaller, manageable chunks.\n",
    "- Embedding Generation: Converting these text chunks into numerical vector representations (gemini-embedding-001).\n",
    "- Vector Store Storage: Storing these embeddings in a vector database for efficient similarity search (e.g., FAISS, Pinecone, Chroma).\n",
    "- Retrieval: Given a user query, finding the most semantically similar document chunks from the vector store.\n",
    "- Augmentation: Passing the retrieved chunks along with the user's query to the LLM.\n",
    "- Generation: The LLM generates a response based on the provided context and the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b348a458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS # Using FAISS as a simple in-memory vector store\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "load_dotenv() # Load your API key from .env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa7a421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM: models/gemini-1.5-pro, Embeddings Model: models/embedding-001\n"
     ]
    }
   ],
   "source": [
    "# --- 0. Setup ---\n",
    "# Initialize LLM and Embedding Model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.2) # Lower temperature for more factual answers in RAG\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# verify if models are loaded\n",
    "print(f\"LLM: {llm.model}, Embeddings Model: {embeddings_model.model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f49b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Document Loading ---\n",
    "# Create a dummy text file for demonstration\n",
    "dummy_text_content = \"\"\"\n",
    "The Amazon River is the largest river by discharge volume of water in the world, and by some definitions, it is the longest.\n",
    "It flows through South America, primarily through Brazil, Peru, and Colombia.\n",
    "Its drainage basin is the largest in the world, about 7.05 million square kilometers (2.72 million square miles).\n",
    "The Amazon rainforest, which it flows through, is the largest tropical rainforest on Earth.\n",
    "Deforestation in the Amazon is a significant environmental concern, impacting biodiversity and climate.\n",
    "Many unique species of flora and fauna, including the pink river dolphin and various types of monkeys, inhabit the Amazon.\n",
    "The river's source is generally considered to be in the Andes Mountains of Peru.\n",
    "\"\"\"\n",
    "# save the dummy text to a file\n",
    "with open(\"amazon_river_info.txt\", \"w\") as f:\n",
    "    f.write(dummy_text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc0aae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s).\n",
      "First 200 chars of document: \n",
      "The Amazon River is the largest river by discharge volume of water in the world, and by some definitions, it is the longest.\n",
      "It flows through South America, primarily through Brazil, Peru, and Colomb...\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(\"amazon_river_info.txt\")\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} document(s).\")\n",
    "print(f\"First 200 chars of document: {documents[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5e80f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split into 2 chunks.\n",
      "First chunk content: The Amazon River is the largest river by discharge volume of water in the world, and by some definitions, it is the longest.\n",
      "It flows through South America, primarily through Brazil, Peru, and Colombia.\n",
      "Its drainage basin is the largest in the world, about 7.05 million square kilometers (2.72 million square miles).\n",
      "The Amazon rainforest, which it flows through, is the largest tropical rainforest on Earth.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Document Splitting (Chunking) ---\n",
    "# RecursiveCharacterTextSplitter is good for maintaining semantic coherence\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, # Max size of each chunk\n",
    "    chunk_overlap=50, # Overlap between chunks to maintain context\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"\\nSplit into {len(chunks)} chunks.\")\n",
    "print(f\"First chunk content: {chunks[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02506649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating vector store from chunks and embeddings...\n",
      "Vector store created successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Embedding Generation & 4. Vector Store Storage ---\n",
    "# Create an in-memory FAISS vector store. For production, use persistent stores like Pinecone, Chroma, etc.\n",
    "print(\"\\nCreating vector store from chunks and embeddings...\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings_model)\n",
    "print(\"Vector store created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1306ff64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retriever created. Ready to fetch relevant chunks.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Retrieval ---\n",
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2}) # Retrieve top 2 most relevant chunks\n",
    "print(\"\\nRetriever created. Ready to fetch relevant chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf44ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Augmentation\n",
    "\n",
    "# Define a prompt template for the LLM that includes context\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system',\"You are a helpful assistant for question-answering tasks. \"\n",
    "                          \"Use the following retrieved context to answer the question. \"\n",
    "                          \"If you don't know the answer, say that you don't know.\"),\n",
    "    ('human', \"Context: {context}\\nQuestion: {question}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d6fe696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RAG Pipeline Ready ---\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Generation (RAG Chain Construction) ---\n",
    "\n",
    "# Create a chain to combine documents for the LLM\n",
    "# This Runnable will format the retrieved documents into a single string for the prompt\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Construct the RAG chain\n",
    "# 1. User question comes in\n",
    "# 2. It's passed to the retriever to get relevant documents (context)\n",
    "# 3. Both the original question and the retrieved context are passed to the prompt\n",
    "# 4. The prompt is sent to the LLM\n",
    "# 5. The LLM's response is parsed\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"\\n--- RAG Pipeline Ready ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "540279f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1: What is considered the source of the Amazon River?\n",
      "Answer 1: The Andes Mountains of Peru are generally considered the source of the Amazon River.\n"
     ]
    }
   ],
   "source": [
    "# --- Test the RAG Pipeline ---\n",
    "\n",
    "# Example 1: Question that can be answered from the provided text\n",
    "question_1 = \"What is considered the source of the Amazon River?\"\n",
    "print(f\"\\nQuestion 1: {question_1}\")\n",
    "response_1 = rag_chain.invoke(question_1)\n",
    "print(f\"Answer 1: {response_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83089a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 2: What are some environmental issues associated with the Amazon?\n",
      "Answer 2: Deforestation is a significant environmental issue associated with the Amazon.\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Question that requires understanding from the context\n",
    "question_2 = \"What are some environmental issues associated with the Amazon?\"\n",
    "print(f\"\\nQuestion 2: {question_2}\")\n",
    "response_2 = rag_chain.invoke(question_2)\n",
    "print(f\"Answer 2: {response_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b708323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 3: Who discovered the Amazon River?\n",
      "Answer 3: The provided text doesn't state who discovered the Amazon River.\n"
     ]
    }
   ],
   "source": [
    "# --- Test Question 3 (same as before) ---\n",
    "question_3 = \"Who discovered the Amazon River?\"\n",
    "print(f\"\\nQuestion 3: {question_3}\")\n",
    "response_3 = rag_chain.invoke(question_3)\n",
    "print(f\"Answer 3: {response_3}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
