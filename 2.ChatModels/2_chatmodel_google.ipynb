{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba480bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install langchain_google_genai library\n",
    "# !pip install langchain_google_genai\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e5aa96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "#check that the environment variable GOOGLE_API_KEY is set\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    raise ValueError(\"GOOGLE_API_KEY environment variable is not set.\") \n",
    "else:\n",
    "    api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
    "    \n",
    "#print(f\"GOOGLE_API_KEY is set: {api_key}\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddf074d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Basic LLM Usage ---\n",
      "The primary function of a CPU (Central Processing Unit) is to **fetch, decode, and execute instructions**.  It's essentially the \"brain\" of the computer, carrying out the calculations and logical operations that make the computer work.  This cycle of fetching, decoding, and executing is continuous, allowing the computer to perform a wide variety of tasks.\n"
     ]
    }
   ],
   "source": [
    "# --- Basic usage (single prompt) ---\n",
    "print(\"--- Basic LLM Usage ---\")\n",
    "result = llm.invoke(\"What is the primary function of a CPU?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a08d0e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using with ChatPromptTemplate and Chains ---\n"
     ]
    }
   ],
   "source": [
    "# --- Using with ChatPromptTemplate and Chains (recommended for structured interactions) ---\n",
    "print(\"\\n--- Using with ChatPromptTemplate and Chains ---\")\n",
    "\n",
    "# 1. Define your prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Answer concisely.\"),\n",
    "    (\"human\", \"{question}\")  # <-- Tuple format for templated message\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1257a706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template:\n",
      "input_variables=['question'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI assistant. Answer concisely.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prompt Template:\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5489ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking Prompt Rendering ---\n",
      "Rendered Messages:\n",
      "Type of rendered_messages: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "Messages in rendered prompt:\n",
      "- System Message: You are a helpful AI assistant. Answer concisely.\n",
      "- Human Message: What is the primary function of a CPU?\n"
     ]
    }
   ],
   "source": [
    "question_to_test = \"What is the primary function of a CPU?\"\n",
    "\n",
    "print(\"--- Checking Prompt Rendering ---\")\n",
    "rendered_messages = prompt.invoke({\"question\": question_to_test})\n",
    "\n",
    "print(\"Rendered Messages:\")\n",
    "print(f\"Type of rendered_messages: {type(rendered_messages)}\") # Should be <class 'langchain_core.messages.BaseMessages'>\n",
    "print(\"Messages in rendered prompt:\")\n",
    "for msg in rendered_messages.messages:\n",
    "    print(f\"- {msg.type.capitalize()} Message: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5631ce7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the primary function of a CPU, elaborate mode in term of interview question?\n",
      "Answer: The CPU (Central Processing Unit) is the primary component of a computer that executes instructions of a computer program.  It fetches instructions from memory, decodes them, performs the specified operations (arithmetic, logical, control), and stores the results back in memory.  Think of it as the \"brain\" of the computer, responsible for carrying out almost all the tasks the computer performs.\n"
     ]
    }
   ],
   "source": [
    "# 2. Define an output parser (optional, but good practice for structured output)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 3. Create a chain\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# 4. Invoke the chain\n",
    "question = \"What is the primary function of a CPU, elaborate mode in term of interview question?\"\n",
    "answer = chain.invoke({\"question\": question})\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3611da6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example with Few-Shot Prompting ---\n",
      "Text: The weather today is surprisingly superhot and humid and I can't take it.\n",
      "Sentiment:Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "# --- Example with a more complex prompt and few-shot examples ---\n",
    "print(\"\\n--- Example with Few-Shot Prompting ---\")\n",
    "few_shot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert sentiment analyzer. Classify the sentiment of the following texts as 'Positive', 'Negative', or 'Neutral'.\"),\n",
    "    (\"human\", \"This movie was fantastic!\"),\n",
    "    (\"human\", \"Positive\"),\n",
    "    (\"human\", \"The service was okay, nothing special.\"),\n",
    "    (\"human\", \"Neutral\"),\n",
    "    (\"human\", \"I absolutely hated the food.\"),\n",
    "    (\"human\", \"Negative\"),\n",
    "    (\"human\", \"{text_to_classify}\")  # <-- Placeholder for the text to classify\n",
    "])\n",
    "\n",
    "sentiment_chain = few_shot_prompt | llm | output_parser\n",
    "\n",
    "text_to_classify = \"The weather today is surprisingly superhot and humid and I can't take it.\"\n",
    "sentiment = sentiment_chain.invoke({\"text_to_classify\": text_to_classify})\n",
    "print(f\"Text: {text_to_classify}\")\n",
    "print(f\"Sentiment:Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "677d2fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The weather today is surprisingly pleasant.\n",
      "Sentiment:Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "text_to_classify = \"The weather today is surprisingly pleasant.\"\n",
    "sentiment = sentiment_chain.invoke({\"text_to_classify\": text_to_classify})\n",
    "print(f\"Text: {text_to_classify}\")\n",
    "print(f\"Sentiment:Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c45e05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
